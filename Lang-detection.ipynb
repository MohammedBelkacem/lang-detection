{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "desperate-example",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tamawt:\n",
    "<SPAN STYLE=\"font-size:14.0pt;font-family:'Calibri'\">Ahilen-a ur teddun ara i tutlayin yettwarun seg uyeffus s azelmaḍ am taɛrabt. Ahulen-a ad ddun kan i tutlayin yettwarun seg uẓelmaḍ s ayeffus akken yebɣu yella unagraw n tira ama d alaṭini am teqbaylit, neɣ tifinaɣ, ... </SPAN>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naval-proportion",
   "metadata": {},
   "source": [
    "# Azdam neɣ asader n tegrummiwin n yisefka:\n",
    "\n",
    "<SPAN STYLE=\"font-size:14.0pt;font-family:'Calibri'\"> Isefka meṛṛa ara nseqdec deg wahil-a, kkan-d seg Tatoeba.\n",
    "Dagi ad nseqdec isefka n tutlayin-a: Taqbaylit, Tafransist, Taspanit, Takatalant, Taglizit, taṭelyanit, Tapurtugit, Talmanit\n",
    "Deg wahil n ddaw-a, ad nseqdec tamkarḍit wget n Python. \n",
    "Deg tazwara, sbedd Python 3.6. Ddu ɣer usmel n Python, sader-d Python 3.6 64 bits i Windows neɣ Linux.\n",
    "\n",
    "Sakin sbedd tamkarḍit wget akka:\n",
    "<b>pip install wget</b>\n",
    "\n",
    "Ur tettut ara d akken tlaq tuqqna Internet i uzdam n yifuyla n tegrummiwin n tutlayin ara tesqedceḍ.\n",
    "\n",
    "- Ifecka:\n",
    "\n",
    "Wid yessnen Jupyter, zemren ad d-sadren afaylu-a seg Github, seg tansa-a:\n",
    "    \n",
    "Wiyaḍ, zemren ad d-kksen ahilen-a, sakin ad ten-selkmen s umesres n PyCharm, PyScripter neɣ Eclipse.</SPAN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "separated-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "# Tafelwit n tutlayin, azdam ad yettwag s useqdec n tengalin-nsent. Tingalin-a d tid n ISO 639-3.\n",
    "# Wali ma tella tutlayt-nniḍen i tebɣiḍ deg Tatoeba tatoeba.org uqbel ad ternuḍ tanglat-a ɣer tfelwit ddaw-a.\n",
    "# Maca, ahil-a i d-iteddun ur iteddu ara i taɛrabt neɣ tutlayin yettwarun seg uyeffus s azelmaḍ\n",
    "tutlayin=['kab','fra','spa','cat','eng','ita','por','ger']\n",
    "\n",
    "# Tabadut n twuri n uzdam n yisefka n tutlayin\n",
    "def zdem (tutlayin):\n",
    "\n",
    " for tutlayt in tutlayin:\n",
    "    # aseɣwen n usader\n",
    "    aseɣwen=\"https://downloads.tatoeba.org/exports/per_language/\"+tutlayt+\"/\"+tutlayt+\"_sentences.tsv.bz2\"\n",
    "    # Isem n ufaylu\n",
    "    isem_ufaylu=tutlayt+\"_sentences.tsv.bz2\"\n",
    "\n",
    "    wget.download(aseɣwen)\n",
    "# asiwel n twuri n uzdams umuddun n wumuɣ n tutlayin i nebɣa d nesleḍ\n",
    "zdem(tutlayin)\n",
    "print ('Azdam ifukk')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-cycle",
   "metadata": {},
   "source": [
    "# Tukksa n usekussem n yifuyla n tegrummiwin n yisefka.\n",
    "Deg windows, semres winrar neɣ winzip.\n",
    "<img src=\"kab.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reasonable-philip",
   "metadata": {},
   "source": [
    "# Aheggi n tegrummiwin\n",
    "Ifuyla i d-nessader seg Tatoeba, gebren kra n telɣut neɣ isallen ur nettiḥwiǧi ara deg wahilen-a. Ihi ilaq ad ten-nekkes. Ddaw-a ad tafem ahil ara yessuffɣen kan tifyar seg yifuyla i d-nessader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tafelwit n tutlayin\n",
    "tutlayin=['kab','fra','spa','cat','eng','ita','por','ger']\n",
    "\n",
    "# Tabadut n twuri ara d-yessuffɣen tifyar seg yal afaylu i d-nessader seg Tatoeba\n",
    "\n",
    "def aheggi_n_tegrummiwin (tutlayin):\n",
    " # yal tutlayt\n",
    " for tutlayt in tutlayin:\n",
    "   # ad d-nawi isem n ufaylu n tutlayt i d-nessader\n",
    "    isem_ufaylu=tutlayt+\"_sentences.tsv\"\n",
    "    #ad d-nedli  ayalu amaynut i tefyar n tutlayt-a\n",
    "    h= open(tutlayt+\".txt\",\"w+\",encoding='utf-8')\n",
    "    # Ad d-nḍum akk iduren n ufaylu. Yal adur tella deg-s tefyirt\n",
    "    for ligne in open(isem_ufaylu,encoding='utf-8'):\n",
    "        a=ligne.split('\\t') # ad nebḍu adur s usekkil n trigla neɣ tabulation\n",
    "        h.write(a[2]) # ad d-nekkes taxxamt tis 3 acku dinna i tella tefyirt. Sakin ad tt-naru deg ufaylu amaynut.\n",
    "    h.close()\n",
    "aheggi_n_tegrummiwin(tutlayin)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "federal-florence",
   "metadata": {},
   "source": [
    "# Asluɣmu n yisefka\n",
    "Ad nseqdec tatiknikt n N-grams akkken ad nselmed n uselkim amek i ttemseḍfaṛen yisekkilen n ugemmay n teqbaylit deg yiḍrisen n teqbaylit.\n",
    "Ddaw-a, ad tafem alguritm n usluɣmu d usnulfu n tneɣruft n uɛqal n tutlayt taqbaylit.\n",
    "Ad nsebded nltk d temkarḍiyin-nniḍen n Python akken ad nizmir adnseqdec tawuri.\n",
    "\n",
    "# Asebeddi n temkarḍiyin\n",
    "Deg tdiwent n Windows neɣ Linux, aru:\n",
    "<b>pip install nltk re codecs numpy string</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-movie",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def train_language(path,lang_name):\n",
    "    words_all = []\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "    # reading the file in unicode format using codecs library\n",
    "    with codecs.open(path,\"r\",\"utf-8\") as filep:\n",
    "\n",
    "        for i,line in enumerate(filep):\n",
    "            # extracting the text sentence from each line\n",
    "            #line = \" \".join(line.split()[1:])\n",
    "            line = line.lower()   # to lower case\n",
    "            line = re.sub(r\"\\d+\", \"\", line) # remove digits\n",
    "\n",
    "            if len(line) != 0:\n",
    "                line = line.translate(translate_table) # remove punctuations\n",
    "                words_all += line\n",
    "                words_all.append(\" \") # append sentences with space\n",
    "\n",
    "    all_str = ''.join(words_all)\n",
    "    all_str = re.sub(' +',' ',all_str) # replace series of spaces with single space\n",
    "    seq_all = [i for i in all_str]\n",
    "\n",
    "    # extracting the bi-grams and sorting them according to their frequencies\n",
    "    finder = BigramCollocationFinder.from_words(seq_all)\n",
    "    finder.apply_freq_filter(5)\n",
    "    bigram_model = finder.ngram_fd.items()\n",
    "    bigram_model = sorted(finder.ngram_fd.items(), key=lambda item: item[1],reverse=True)\n",
    "\n",
    "    print (lang_name)\n",
    "    for i in bigram_model:\n",
    "        print (i)\n",
    "\n",
    "    np.save(lang_name+\".npy\",bigram_model) # asekles n tneɣruft n tutlayt\n",
    "lang_name = ['kab','fra','spa','cat','eng','ita','por']\n",
    "train_lang_path = ['kab.txt','fra.txt','spa.txt','cat.txt','eng.txt','ita.txt','por.txt']\n",
    "root=''\n",
    "for i,p in enumerate(train_lang_path):\n",
    "        train_language(root+p,lang_name[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "necessary-connectivity",
   "metadata": {},
   "source": [
    "# Asemres n tneɣruft n tutlayt\n",
    "Ticki tella tneɣruft n tutlayt, aselkim ad yizmir i yiman-is ad d-yaf n wanta-tt tutlayt ideg ttwarunt tefyar. Aya, ad yettwaseqdec deg waṭas n tɣula am usismel awurman n yidlisen, imagraden, ... atg.\n",
    "Maca daɣen, ad nizmir ad nsismel tiyar s tutlayin yemgaraden xas ma xelḍent deg yiwen n yisemli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governmental-terrace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import BigramCollocationFinder\n",
    "import re\n",
    "import codecs\n",
    "import numpy as np\n",
    "import string\n",
    "\n",
    "def test_language(path,language,total):\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    lang_name = ['kab','fra','spa','cat','eng','ita','por']\n",
    "    model = [np.load(lang+\".npy\",allow_pickle=True) for lang in lang_name]\n",
    "\n",
    "    with codecs.open(path,\"r\",\"utf-8\") as filep:\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "        for l,line in enumerate(filep):\n",
    "\n",
    "            line = line.lower() # Ad nerr meṛṛa isekkilen d imeẓẓyanen\n",
    "            line = re.sub(r\"\\d+\", \"\", line)# tukksa n yizwilen\n",
    "            line = line.translate(translate_table) #tikksa n usenqeḍ\n",
    "\n",
    "            finder = BigramCollocationFinder.from_words(line)\n",
    "\n",
    "            freq_sum = np.zeros(len(lang_name))# 8 n tutlayin\n",
    "            for k,v in finder.ngram_fd.items():\n",
    "                isthere = 0\n",
    "                for i,lang in enumerate(lang_name):\n",
    "                    for key,f in model[i]:\n",
    "                        if k == key:\n",
    "                            freq_sum[i] = freq_sum[i]+(f*10000)/total[i]\n",
    "                            isthere = 1\n",
    "                            break\n",
    "                    if isthere == 0:\n",
    "                        freq_sum[i] = freq_sum[i] + 1\n",
    "\n",
    "            max_val = freq_sum.max()\n",
    "            index= freq_sum.argmax()\n",
    "            if max_val != 0:\n",
    "                if lang_name[index] == language:\n",
    "                    tp = tp + 1\n",
    "                else:\n",
    "                    fp = fp + 1\n",
    "            print (\"tp = \",tp,\"fp = \",fp,\" Taseqqart tafellayt =\", np.max(freq_sum),\" Tafelwit n tseqqar: \",freq_sum )\n",
    "            print(line)\n",
    "    #print (\"True Positive = \",tp)\n",
    "    #print (\"False Positive = \",fp )\n",
    "\n",
    "\n",
    "root = \"\"\n",
    "lang_name = ['kab','fra','spa','cat','eng','ita','por']\n",
    "no_of_bigms = []\n",
    "for i,lang in enumerate(lang_name):\n",
    "        model = np.load(lang+\".npy\",allow_pickle=True)\n",
    "        total = 0\n",
    "        for key,v in model:\n",
    "            total = total + v\n",
    "        no_of_bigms.append(total)\n",
    "        #print (total)\n",
    "\n",
    "train_lang_path = ['testkab.txt','testfra.txt','testspa.txt','testcat.txt','testeng.txt','testita.txt']\n",
    "for i,p in enumerate(train_lang_path):\n",
    "  if lang_name[i]==\"kab\":\n",
    "         print (\"Testing of \",lang_name[i])\n",
    "         test_language(root+p,lang_name[i],no_of_bigms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-delicious",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
